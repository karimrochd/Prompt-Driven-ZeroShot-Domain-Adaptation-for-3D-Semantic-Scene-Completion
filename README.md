# PÃ˜DA-MonoScene: Zero-Shot Domain Adaptation for 3D Semantic Scene Completion



> **Integrating Prompt-driven Zero-shot Domain Adaptation (PÃ˜DA) with MonoScene for robust 3D scene understanding under adverse weather conditions**

This project combines two state-of-the-art methods to enable **zero-shot domain adaptation** of 3D Semantic Scene Completion (SSC) models to adverse weather conditions (fog, rain, snow) using only **text prompts**â€”no target domain images required during training.


## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Contributions](#-key-contributions)
- [Method](#-method)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Dataset Setup](#-dataset-setup)
- [Training](#-training)
- [Evaluation](#-evaluation)
- [Results](#-results)
- [References](#-references)

## ğŸ¯ Overview

### Problem Statement
3D Semantic Scene Completion (SSC) models trained on clear weather data suffer significant performance degradation when deployed in adverse conditions (fog, rain, snow). Traditional domain adaptation requires collecting and labeling target domain data, which is expensive and sometimes dangerous.

### My Solution
I integrate **PÃ˜DA** (Prompt-driven Zero-shot Domain Adaptation) with **MonoScene** (Monocular 3D SSC) to adapt a source-trained model using only natural language descriptions of target conditions:
- `"driving in fog"`
- `"driving under rain"`  
- `"driving in snow"`

### Papers
- **PÃ˜DA**: [Prompt-driven Zero-shot Domain Adaptation](https://arxiv.org/abs/2212.03241) (Fahes et al., ICCV 2023)
- **MonoScene**: [Monocular 3D Semantic Scene Completion](https://arxiv.org/abs/2112.00726) (Cao & de Charette, CVPR 2022)

## ğŸŒŸ Key Contributions

1. **First integration** of prompt-driven domain adaptation with 3D semantic scene completion
2. **CLIP-based backbone replacement**: Replace MonoScene's EfficientNetB7 with CLIP ResNet-50 to enable vision-language alignment
3. **PIN layer insertion**: Apply Prompt-driven Instance Normalization after Layer1 for optimal style transfer
4. **Unified style mining**: Mine and combine styles from multiple adverse weather domains
5. **Zero-shot adaptation**: Adapt to fog/rain/snow without any target domain images

## ğŸ”¬ Method

### Overview

The method consists of three phases:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PÃ˜DA + MonoScene Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Phase 1: Source-Only Training                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  RGB    â”‚â”€â”€â”€â–¶â”‚  CLIP   â”‚â”€â”€â”€â–¶â”‚  FLoSP  â”‚â”€â”€â”€â–¶â”‚  3D     â”‚â”€â”€â”€â–¶ SSC     â”‚
â”‚  â”‚  Image  â”‚    â”‚  RN50   â”‚    â”‚         â”‚    â”‚  UNet   â”‚    Output    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                         â”‚
â”‚  Phase 2: Style Mining (Offline)                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  For each target prompt ("driving in fog", "driving in rain"):  â”‚   â”‚
â”‚  â”‚  â€¢ Extract Layer1 features from source images                    â”‚   â”‚
â”‚  â”‚  â€¢ Optimize (Î¼, Ïƒ) to minimize cosine distance to text embeddingâ”‚   â”‚
â”‚  â”‚  â€¢ Store mined styles in style bank                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  Phase 3: Zero-Shot Adaptation                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  RGB    â”‚â”€â”€â”€â–¶â”‚  CLIP   â”‚â”€â”€â”€â–¶â”‚   PIN   â”‚â”€â”€â”€â–¶â”‚  Rest   â”‚â”€â”€â”€â–¶ SSC     â”‚
â”‚  â”‚  Image  â”‚    â”‚ Layer1  â”‚    â”‚  Layer  â”‚    â”‚  of Net â”‚    Output    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                      â”‚              â–²                                   â”‚
â”‚                      â”‚              â”‚ Sample random style               â”‚
â”‚                      â”‚         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                              â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Style  â”‚                              â”‚
â”‚                                â”‚  Bank   â”‚                              â”‚
â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prompt-driven Instance Normalization (PIN)

PIN transforms source features toward target domain style:

```
f_{sâ†’t} = Ïƒ_t * ((f_s - Î¼(f_s)) / Ïƒ(f_s)) + Î¼_t
```

Where:
- `f_s`: Source feature map from Layer1 (shape: B Ã— 256 Ã— H Ã— W)
- `Î¼(f_s), Ïƒ(f_s)`: Channel-wise mean and std of source features
- `Î¼_t, Ïƒ_t`: Target style statistics (optimized via CLIP)

### Style Mining

For each source image, we optimize style statistics to minimize:

```
L(Î¼, Ïƒ) = 1 - cos(fÌ„_{sâ†’t}, TrgEmb)
```

Where:
- `fÌ„_{sâ†’t}`: CLIP embedding of stylized features
- `TrgEmb`: CLIP text embedding of target prompt (e.g., "driving in fog")

## ğŸ— Architecture

### Model Components

```
MonoScenePODA
â”œâ”€â”€ CLIPBackbone (frozen)
â”‚   â”œâ”€â”€ stem (conv1-3, avgpool)
â”‚   â”œâ”€â”€ layer1 â†’ 256 channels  â† PIN insertion point
â”‚   â”œâ”€â”€ layer2 â†’ 512 channels
â”‚   â”œâ”€â”€ layer3 â†’ 1024 channels
â”‚   â””â”€â”€ layer4 â†’ 2048 channels
â”‚
â”œâ”€â”€ PIN Layer
â”‚   â””â”€â”€ Prompt-driven Instance Normalization
â”‚
â”œâ”€â”€ FLoSP (Features Line of Sight Projection)
â”‚   â”œâ”€â”€ 1x1 conv projections for each scale
â”‚   â””â”€â”€ 2Dâ†’3D feature lifting via ray casting
â”‚
â”œâ”€â”€ 3D UNet
â”‚   â”œâ”€â”€ Encoder (DDR blocks, 2 layers)
â”‚   â”œâ”€â”€ 3D CRP (Context Relation Prior)
â”‚   â””â”€â”€ Decoder (deconv layers)
â”‚
â””â”€â”€ Completion Head
    â”œâ”€â”€ 3D ASPP (dilations: 1, 2, 3)
    â””â”€â”€ Softmax â†’ 20 classes
```

### Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Backbone | CLIP RN50 | Vision-language alignment for PÃ˜DA |
| PIN Location | After Layer1 | Low-level features encode style; high-level encode content |
| Freeze Strategy | Freeze all except Completion Head | Preserve CLIP latent space compatibility |

## ğŸ“ Dataset Setup

### SemanticKITTI (Source Domain)

```
data/SemanticKITTI/
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ sequences/
â”‚       â”œâ”€â”€ 00/
â”‚       â”‚   â”œâ”€â”€ image_2/
â”‚       â”‚   â”œâ”€â”€ calib.txt
â”‚       â”‚   â””â”€â”€ voxels/
â”‚       â”‚       â”œâ”€â”€ 000000.bin
â”‚       â”‚       â”œâ”€â”€ 000000.label
â”‚       â”‚       â””â”€â”€ 000000.invalid
â”‚       â”œâ”€â”€ 01/ ... 10/
â”‚       â””â”€â”€ 08/  (validation)
â””â”€â”€ semantic-kitti.yaml
```


### SemanticSTF (Target Domain - Evaluation Only)

```
data/SemanticSTF/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ rgb/
â”‚   â”œâ”€â”€ voxel/
â”‚   â””â”€â”€ calib/
â””â”€â”€ weather_split.json
```


## ğŸ“ˆ Results

### SemanticSTF Evaluation (Zero-Shot Transfer)

| Weather | Source SC-IoU | Adapted SC-IoU | Î” IoU | Source mIoU | Adapted mIoU | Î” mIoU |
|---------|--------------|----------------|-------|-------------|--------------|--------|
| Snow | 20.95% | **41.84%** | +20.89% | 2.49% | **4.44%** | +1.95% |
| Rain | 15.66% | **39.87%** | +24.21% | 2.27% | **4.60%** | +2.33% |
| Dense Fog | 16.39% | **47.50%** | +31.11% | 2.41% | **6.07%** | +3.66% |
| Light Fog | 20.04% | **50.76%** | +30.72% | 2.67% | **5.03%** | +2.36% |

**Key Observations:**
- **Significant SC-IoU improvements**: +20-31% across all weather conditions
- **Consistent mIoU gains**: +2-4% semantic accuracy improvement
- **Best performance on fog**: Dense fog shows largest improvement (+31% IoU)
- **Zero-shot transfer**: No target domain images used during training

### SemanticKITTI Validation (Source Domain Retention)

| Metric | Value |
|--------|-------|
| SC-IoU | 16.97% |
| mIoU | 9.33% |

## ğŸ“š References

```bibtex
@inproceedings{fahes2023poda,
  title={PÃ˜DA: Prompt-driven Zero-shot Domain Adaptation},
  author={Fahes, Mohammad and Vu, Tuan-Hung and Bursuc, Andrei and P{\'e}rez, Patrick and de Charette, Raoul},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{cao2022monoscene,
  title={MonoScene: Monocular 3D Semantic Scene Completion},
  author={Cao, Anh-Quan and de Charette, Raoul},
  booktitle={CVPR},
  year={2022}
}


```

## ğŸ™ Acknowledgments

- [PÃ˜DA](https://github.com/astra-vision/PODA) by Astra-Vision
- [MonoScene](https://github.com/astra-vision/MonoScene) by Astra-Vision
- [CLIP](https://github.com/openai/CLIP) by OpenAI
- [SemanticKITTI](http://www.semantic-kitti.org/)
- [SemanticSTF](https://github.com/xiaoaoran/SemanticSTF)

